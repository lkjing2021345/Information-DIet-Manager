"""
æ–‡æœ¬åˆ†ç±»å™¨æ¨¡å—

åŠŸèƒ½æ¦‚è¿°ï¼š
    å¯¹æµè§ˆè®°å½•çš„æ ‡é¢˜/URLè¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œåˆ¤æ–­ç”¨æˆ·è®¿é—®çš„æ˜¯æ–°é—»ã€å¨±ä¹ã€å­¦ä¹ ç­‰å“ªç±»å†…å®¹

ä¸»è¦æŠ€æœ¯ï¼š
    - jieba: ä¸­æ–‡åˆ†è¯
    - sklearn: TF-IDF ç‰¹å¾æå– + æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

å­¦ä¹ è¦ç‚¹ï¼š
    - ç±»çš„å°è£…è®¾è®¡
    - è§„åˆ™åŒ¹é… vs æœºå™¨å­¦ä¹ æ–¹æ³•çš„é€‰æ‹©
    - æ–‡æœ¬é¢„å¤„ç†æµç¨‹
"""

import jieba
import pandas as pd
import os
import pickle
import logging
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# ==================== å¯é€‰å¯¼å…¥ ====================
# Day 4 ä¹‹åå–æ¶ˆæ³¨é‡Šï¼Œç”¨äºæœºå™¨å­¦ä¹ åˆ†ç±»
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report

# logger åŸºæœ¬è®¾ç½®
logs_folder_path = "../../logs"
if not os.path.exists(logs_folder_path):
    os.makedirs(logs_folder_path)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('../../logs/classifier.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class ContentClassifier:
    """
    æµè§ˆè®°å½•å†…å®¹åˆ†ç±»å™¨

    è®¾è®¡æ€è·¯ï¼š
        é‡‡ç”¨"è§„åˆ™ä¼˜å…ˆï¼Œæ¨¡å‹å…œåº•"çš„æ··åˆç­–ç•¥ï¼š
        1. å…ˆç”¨å…³é”®è¯è§„åˆ™å¿«é€ŸåŒ¹é…ï¼ˆå‡†ç¡®ç‡é«˜ã€é€Ÿåº¦å¿«ï¼‰
        2. è§„åˆ™æœªå‘½ä¸­æ—¶ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆæ³›åŒ–èƒ½åŠ›å¼ºï¼‰
        3. éƒ½å¤±è´¥åˆ™å½’ç±»ä¸º Other

    å±æ€§è¯´æ˜ï¼š
        categories: æ”¯æŒçš„åˆ†ç±»ç±»åˆ«åˆ—è¡¨
        rules: å…³é”®è¯è§„åˆ™å­—å…¸ {ç±»åˆ«: [å…³é”®è¯åˆ—è¡¨]}
        model: æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆæœ´ç´ è´å¶æ–¯ï¼‰
        vectorizer: TF-IDF å‘é‡åŒ–å™¨
    """

    # ==================== ç±»å¸¸é‡ ====================
    # ä½¿ç”¨ç±»å¸¸é‡å®šä¹‰é»˜è®¤ç±»åˆ«ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†
    CATEGORY_NEWS = "News"           # æ–°é—»
    CATEGORY_ENTERTAINMENT = "Entertainment"  # å¨±ä¹
    CATEGORY_LEARNING = "Learning"   # å­¦ä¹ 
    CATEGORY_SOCIAL = "Social"        # ç¤¾äº¤
    CATEGORY_SHOPPING = "Shopping"    # è´­ç‰©
    CATEGORY_TOOLS = "Tools"          # å·¥å…·
    CATEGORY_OTHER = "Other"          # å…¶ä»–

    # ==================== åˆå§‹åŒ–æ–¹æ³• ====================

    def __init__(self,
                 keyword_dict: Optional[Dict[str, List[str]]] = None,
                 model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨

        å‚æ•°:
            keyword_dict: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼Œæ ¼å¼ä¸º {'ç±»åˆ«': ['è¯1', 'è¯2']}
                          å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤è§„åˆ™åº“
            model_path: å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨åŠ è½½

        å­¦ä¹ è¦ç‚¹:
            - Optional ç±»å‹æç¤ºè¡¨ç¤ºå‚æ•°å¯ä»¥ä¸º None
            - __init__ æ–¹æ³•ä¸åº”åŒ…å«è€—æ—¶æ“ä½œ
        """
        # åˆå§‹åŒ–åˆ†ç±»ç±»åˆ«åˆ—è¡¨
        self.categories = []

        # åŠ è½½å…³é”®è¯è§„åˆ™åº“
        self.rules = keyword_dict if keyword_dict is not None else self._load_default_rules()

        self.categories = list(self.rules.keys())

        # TODO: åˆå§‹åŒ–æœºå™¨å­¦ä¹ ç›¸å…³å±æ€§ï¼ˆåˆå§‹ä¸º Noneï¼‰
        self.model = None       # æœ´ç´ è´å¶æ–¯æ¨¡å‹
        self.vectorizer = None  # TF-IDF å‘é‡åŒ–å™¨

        # TODO: å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œå°è¯•åŠ è½½æ¨¡å‹
        # æç¤ºï¼šè°ƒç”¨ self.load_model(model_path)

        logger.info("âœ… ContentClassifier åˆå§‹åŒ–å®Œæˆ")

    # ==================== ç§æœ‰æ–¹æ³•ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰====================

    def _load_default_rules(self) -> Dict[str, List[str]]:
        """
        åŠ è½½é»˜è®¤çš„å…³é”®è¯è§„åˆ™åº“

        è¿”å›:
            Dict[str, List[str]]: å…³é”®è¯è§„åˆ™å­—å…¸
        """

        current_dir = Path(__file__).parent
        json_dir = current_dir.joinpath("rules")
        config_path = json_dir.joinpath("default_classify_rules.json")

        try:
            with open(config_path, 'r', encoding="utf-8") as f:
                raw_data = json.load(f)

            category_mapping = {
                "Social": self.CATEGORY_SOCIAL,
                "Learning": self.CATEGORY_LEARNING,
                "Shopping": self.CATEGORY_SHOPPING,
                "Entertainment": self.CATEGORY_ENTERTAINMENT,
                "News": self.CATEGORY_NEWS,
                "Tools": self.CATEGORY_TOOLS,
                "Other": self.CATEGORY_OTHER,
            }

            result = {}
            for key, category_const in category_mapping.items():
                if key in raw_data:
                    result[category_const] = raw_data[key]
                else:
                    result[category_const] = []

            return result

        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç©ºè§„åˆ™")
            return {}

        except json.JSONDecodeError:
            logger.error(f"é…ç½®æ–‡ä»¶ {config_path} æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥ JSON è¯­æ³•")
            return {}

        except Exception as e:
            logger.error(f"å‡ºç°å¼‚å¸¸é”™è¯¯: {e}")
            return {}

    def _segment_text(self, text: str) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯

        å‚æ•°:
            text: å¾…åˆ†è¯çš„æ–‡æœ¬å­—ç¬¦ä¸²

        è¿”å›:
            List[str]: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨

        å­¦ä¹ è¦ç‚¹:
            - jieba.cut() è¿”å›ç”Ÿæˆå™¨ï¼Œéœ€è¦ç”¨ list() è½¬æ¢
            - jieba.lcut() ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œæ›´æ–¹ä¾¿
            - å¯ä»¥åŠ è½½è‡ªå®šä¹‰è¯å…¸æé«˜åˆ†è¯å‡†ç¡®ç‡

        jieba å¸¸ç”¨æ–¹æ³•:
            - jieba.cut(text): ç²¾ç¡®æ¨¡å¼åˆ†è¯
            - jieba.lcut(text): è¿”å›åˆ—è¡¨
            - jieba.add_word(word): æ·»åŠ è‡ªå®šä¹‰è¯
            - jieba.load_userdict(path): åŠ è½½è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶
        """
        if text is None:
            logger.error("è¾“å…¥çš„æ–‡æœ¬ä¸º None")
            return []

        try:
            words = jieba.lcut(text)
            return words

        except Exception as e:
            logger.exception(f"åˆ†è¯å¤±è´¥: {e}")
            return []


    def _remove_stopwords(self, words: List[str]) -> List[str]:
        """
        ç§»é™¤åœç”¨è¯

        å‚æ•°:
            words: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨

        è¿”å›:
            List[str]: ç§»é™¤åœç”¨è¯åçš„è¯è¯­åˆ—è¡¨

        è¯´æ˜:
            åœç”¨è¯æ˜¯æŒ‡"çš„"ã€"æ˜¯"ã€"åœ¨"ç­‰æ— å®é™…æ„ä¹‰çš„è¯è¯­
            ç§»é™¤åœç”¨è¯å¯ä»¥æé«˜åˆ†ç±»å‡†ç¡®ç‡

        TODO:
            1. å®šä¹‰åœç”¨è¯åˆ—è¡¨æˆ–ä»æ–‡ä»¶åŠ è½½
            2. è¿‡æ»¤æ‰åœç”¨è¯
        """
        pass

    def _extract_domain(self, url: str) -> str:
        """
        ä» URL ä¸­æå–åŸŸå

        å‚æ•°:
            url: å®Œæ•´çš„ URL å­—ç¬¦ä¸²

        è¿”å›:
            str: åŸŸåéƒ¨åˆ†ï¼Œå¦‚ "www.baidu.com"

        æç¤º:
            - å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²çš„ split('/') æ–¹æ³•
            - æˆ–ä½¿ç”¨ urllib.parse.urlparse() è§£æ

        ç¤ºä¾‹:
            è¾“å…¥: "https://www.bilibili.com/video/xxx"
            è¾“å‡º: "www.bilibili.com" æˆ– "bilibili.com"
        """
        # TODO: å®ç°åŸŸåæå–
        pass

    def _predict_by_model(self, text: str) -> str:
        """
        ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹

        å‚æ•°:
            text: å¾…é¢„æµ‹çš„æ–‡æœ¬

        è¿”å›:
            str: é¢„æµ‹çš„ç±»åˆ«

        å‰ç½®æ¡ä»¶:
            self.model å’Œ self.vectorizer å¿…é¡»å·²è®­ç»ƒ

        å®ç°æ­¥éª¤:
            1. å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œé¢„å¤„ç†
            2. ä½¿ç”¨ vectorizer è½¬æ¢ä¸º TF-IDF å‘é‡
            3. ä½¿ç”¨ model.predict() é¢„æµ‹ç±»åˆ«
        """
        # TODO: Day 4 å®ç°æœºå™¨å­¦ä¹ é¢„æµ‹
        pass

    # ==================== æ ¸å¿ƒå…¬å…±æ–¹æ³• ====================

    def predict_by_rules(self, text: str, url: Optional[str] = None) -> Optional[str]:
        """
        åŸºäºå…³é”®è¯è§„åˆ™è¿›è¡Œåˆ†ç±»

        å‚æ•°:
            text: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µ URLï¼ˆå¯é€‰ï¼Œè¾…åŠ©åˆ¤æ–­ï¼‰

        è¿”å›:
            Optional[str]: åŒ¹é…åˆ°çš„ç±»åˆ«ï¼ŒæœªåŒ¹é…è¿”å› None

        è®¾è®¡æ€è·¯:
            1. URL åŒ¹é…ä¼˜å…ˆï¼ˆåŸŸåæ›´å‡†ç¡®ï¼‰
            2. æ ‡é¢˜å…³é”®è¯åŒ¹é…æ¬¡ä¹‹
            3. åªè¦åŒ¹é…åˆ°å°±è¿”å›ï¼Œä¸è¿›è¡Œå¤šç±»åˆ«åˆ¤æ–­

        æç¤º:
            - å­—ç¬¦ä¸²çš„ in æ“ä½œç¬¦å¯ä»¥åˆ¤æ–­å­ä¸²
            - ä½¿ç”¨ .lower() ç»Ÿä¸€è½¬ä¸ºå°å†™ï¼Œæé«˜åŒ¹é…ç‡
        """
        text_lower = str(text).lower() if text else ""
        url_lower = str(url).lower() if url else ""

        if url_lower:
            for category, keywords in self.rules.items():
                for keyword in keywords:
                    if keyword.lower() in keywords:
                        logger.info(f"urlç±»åˆ«æˆåŠŸåŒ¹é…: {category}")
                        return category

        logger.warning(f"urlç±»åˆ«åŒ¹é…å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ ‡é¢˜å…³é”®è¯åŒ¹é…")
        if text_lower:
            for category, keywords in self.rules.items():
                for keyword in keywords:
                    if keyword.lower() in text_lower:
                        logger.info(f"æ ‡é¢˜å…³é”®å­—åŒ¹é…æˆåŠŸ: {category}")
                        return category

        logger.warning("è§„åˆ™åŒ¹é…å¤±è´¥")
        return None

    def train_model(self,
                    texts: List[str],
                    labels: List[str],
                    test_size: float = 0.2) -> Dict[str, float]:
        """
        è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

        å‚æ•°:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            labels: å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨
            test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼Œé»˜è®¤ 0.2

        è¿”å›:
            Dict[str, float]: åŒ…å«å‡†ç¡®ç‡ç­‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸

        å®ç°æ­¥éª¤:
            1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆåˆ†è¯ã€å»åœç”¨è¯ï¼‰
            2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            3. åˆ›å»º TfidfVectorizer å¹¶è½¬æ¢æ–‡æœ¬
            4. è®­ç»ƒ MultinomialNB æ¨¡å‹
            5. è¯„ä¼°æ¨¡å‹æ€§èƒ½

        sklearn å…³é”®æ–¹æ³•:
            - TfidfVectorizer(): åˆ›å»º TF-IDF å‘é‡åŒ–å™¨
              - fit_transform(texts): æ‹Ÿåˆå¹¶è½¬æ¢
              - transform(texts): ä»…è½¬æ¢ï¼ˆç”¨äºæ–°æ•°æ®ï¼‰
            - MultinomialNB(): æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
              - fit(X, y): è®­ç»ƒæ¨¡å‹
              - predict(X): é¢„æµ‹
              - score(X, y): è®¡ç®—å‡†ç¡®ç‡
            - train_test_split(): åˆ’åˆ†æ•°æ®é›†

        TODO: Day 4 å®ç°æ¨¡å‹è®­ç»ƒ
        """
        print("ğŸ”„ æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
        # TODO: å®ç°è®­ç»ƒé€»è¾‘
        pass

    def predict(self, text: str, url: Optional[str] = None) -> str:
        """
        é¢„æµ‹å•æ¡æ–‡æœ¬çš„ç±»åˆ«ï¼ˆä¸»å…¥å£æ–¹æ³•ï¼‰

        å‚æ•°:
            text: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µ URLï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            str: é¢„æµ‹çš„ç±»åˆ«

        åˆ†ç±»ç­–ç•¥:
            1. ä¼˜å…ˆä½¿ç”¨è§„åˆ™åŒ¹é…ï¼ˆå¿«é€Ÿã€å‡†ç¡®ï¼‰
            2. è§„åˆ™æœªå‘½ä¸­ä¸”æœ‰æ¨¡å‹æ—¶ï¼Œä½¿ç”¨æ¨¡å‹é¢„æµ‹
            3. éƒ½å¤±è´¥åˆ™è¿”å› Other

        è¿™æ˜¯ç±»æœ€é‡è¦çš„å¯¹å¤–æ¥å£ï¼
        """
        # TODO: å®ç°é¢„æµ‹é€»è¾‘
        # æç¤ºï¼šè°ƒç”¨ self.predict_by_rules() å’Œ self._predict_by_model()
        pass

    def batch_predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ‰¹é‡é¢„æµ‹ DataFrame ä¸­çš„æ•°æ®

        å‚æ•°:
            df: åŒ…å« 'title' å’Œ 'url' åˆ—çš„ DataFrame

        è¿”å›:
            pd.DataFrame: æ·»åŠ äº† 'category' åˆ—çš„ DataFrame

        Pandas æŠ€å·§:
            - df.apply(func, axis=1): å¯¹æ¯è¡Œåº”ç”¨å‡½æ•°
            - df['col'].apply(func): å¯¹å•åˆ—åº”ç”¨å‡½æ•°
            - ä½¿ç”¨è¿›åº¦æ¡åº“ tqdm å¯ä»¥æ˜¾ç¤ºå¤„ç†è¿›åº¦

        æ€§èƒ½ä¼˜åŒ–å»ºè®®:
            - å¯¹äºå¤§é‡æ•°æ®ï¼Œå¯ä»¥è€ƒè™‘å‘é‡åŒ–æ“ä½œ
            - æˆ–è€…ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        """
        if df.empty:
            print("âš ï¸ è¾“å…¥æ•°æ®ä¸ºç©º")
            return df

        print(f"ğŸ“Š æ­£åœ¨å¤„ç† {len(df)} æ¡æ•°æ®...")

        # TODO: å®ç°æ‰¹é‡é¢„æµ‹
        # æç¤ºï¼šä½¿ç”¨ df.apply(lambda row: self.predict(...), axis=1)

        return df

    # ==================== æ¨¡å‹æŒä¹…åŒ–æ–¹æ³• ====================

    def save_model(self, path: str) -> None:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æ–‡ä»¶

        å‚æ•°:
            path: æ¨¡å‹ä¿å­˜è·¯å¾„

        è¯´æ˜:
            æ¨¡å‹ä¿å­˜åï¼Œä¸‹æ¬¡å¯åŠ¨å¯ä»¥ç›´æ¥åŠ è½½ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ

        Python æŒä¹…åŒ–æ–¹æ³•:
            - pickle.dump(obj, file): åºåˆ—åŒ–å¯¹è±¡
            - pickle.load(file): ååºåˆ—åŒ–å¯¹è±¡
            - ä¹Ÿå¯ä»¥ä½¿ç”¨ joblibï¼ˆsklearn æ¨èï¼‰

        éœ€è¦ä¿å­˜çš„å†…å®¹:
            - self.model (åˆ†ç±»å™¨)
            - self.vectorizer (å‘é‡åŒ–å™¨)
            - self.categories (ç±»åˆ«åˆ—è¡¨)
        """
        # TODO: å®ç°æ¨¡å‹ä¿å­˜
        # æç¤ºï¼š
        # with open(path, 'wb') as f:
        #     pickle.dump({...}, f)
        pass

    def load_model(self, path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹

        å‚æ•°:
            path: æ¨¡å‹æ–‡ä»¶è·¯å¾„

        è¿”å›:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ

        æ³¨æ„:
            åŠ è½½å‰æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        # TODO: å®ç°æ¨¡å‹åŠ è½½
        pass

    def get_category_distribution(self, df: pd.DataFrame) -> pd.Series:
        """
        ç»Ÿè®¡åˆ†ç±»ç»“æœçš„åˆ†å¸ƒæƒ…å†µ

        å‚æ•°:
            df: åŒ…å« 'category' åˆ—çš„ DataFrame

        è¿”å›:
            pd.Series: å„ç±»åˆ«çš„æ•°é‡ç»Ÿè®¡

        ç”¨é€”:
            ç”¨äºåˆ†æç”¨æˆ·æµè§ˆä¹ æƒ¯ï¼Œç”ŸæˆæŠ¥å‘Š

        Pandas æ–¹æ³•:
            - df['col'].value_counts(): ç»Ÿè®¡å„å€¼å‡ºç°æ¬¡æ•°
        """
        # TODO: å®ç°ç»Ÿè®¡é€»è¾‘
        pass


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    """
    å•å…ƒæµ‹è¯•ï¼šç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ¥æµ‹è¯•åˆ†ç±»å™¨åŠŸèƒ½
    
    æµ‹è¯•æ­¥éª¤:
        1. å®ä¾‹åŒ–åˆ†ç±»å™¨
        2. æµ‹è¯•å•æ¡æ–‡æœ¬é¢„æµ‹
        3. æµ‹è¯•æ‰¹é‡é¢„æµ‹
        4. (å¯é€‰) æµ‹è¯•æ¨¡å‹è®­ç»ƒå’Œä¿å­˜
    """

    print("=" * 50)
    print("ContentClassifier å•å…ƒæµ‹è¯•")
    print("=" * 50)

    # TODO: ç¼–å†™æµ‹è¯•ä»£ç 

    # 1. å®ä¾‹åŒ–åˆ†ç±»å™¨
    classifier = ContentClassifier()

    # 2. æµ‹è¯•å•æ¡é¢„æµ‹
    # test_cases = [
    #     {"title": "Python åŸºç¡€æ•™ç¨‹", "url": "https://www.runoob.com/python"},
    #     {"title": "äº¬ä¸œ - æ­£å“ä½ä»·", "url": "https://www.jd.com"},
    #     ...
    # ]
    # for case in test_cases:
    #     result = classifier.predict(case['title'], case['url'])
    #     print(f"æ ‡é¢˜: {case['title']} -> {result}")

    # 3. æµ‹è¯•æ‰¹é‡é¢„æµ‹
    # df = pd.DataFrame(test_cases)
    # result_df = classifier.batch_predict(df)
    # print(result_df)

    print("\nâœ… æµ‹è¯•å®Œæˆ")