"""
æ–‡æœ¬åˆ†ç±»å™¨æ¨¡å—

åŠŸèƒ½æ¦‚è¿°ï¼š
    å¯¹æµè§ˆè®°å½•çš„æ ‡é¢˜/URLè¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œåˆ¤æ–­ç”¨æˆ·è®¿é—®çš„æ˜¯æ–°é—»ã€å¨±ä¹ã€å­¦ä¹ ç­‰å“ªç±»å†…å®¹

ä¸»è¦æŠ€æœ¯ï¼š
    - jieba: ä¸­æ–‡åˆ†è¯
    - sklearn: TF-IDF ç‰¹å¾æå– + æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

å­¦ä¹ è¦ç‚¹ï¼š
    - ç±»çš„å°è£…è®¾è®¡
    - è§„åˆ™åŒ¹é… vs æœºå™¨å­¦ä¹ æ–¹æ³•çš„é€‰æ‹©
    - æ–‡æœ¬é¢„å¤„ç†æµç¨‹
"""

import jieba
import pandas as pd
import os
import pickle
from typing import List, Dict, Optional, Tuple

# ==================== å¯é€‰å¯¼å…¥ ====================
# Day 4 ä¹‹åå–æ¶ˆæ³¨é‡Šï¼Œç”¨äºæœºå™¨å­¦ä¹ åˆ†ç±»
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report


class ContentClassifier:
    """
    æµè§ˆè®°å½•å†…å®¹åˆ†ç±»å™¨

    è®¾è®¡æ€è·¯ï¼š
        é‡‡ç”¨"è§„åˆ™ä¼˜å…ˆï¼Œæ¨¡å‹å…œåº•"çš„æ··åˆç­–ç•¥ï¼š
        1. å…ˆç”¨å…³é”®è¯è§„åˆ™å¿«é€ŸåŒ¹é…ï¼ˆå‡†ç¡®ç‡é«˜ã€é€Ÿåº¦å¿«ï¼‰
        2. è§„åˆ™æœªå‘½ä¸­æ—¶ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆæ³›åŒ–èƒ½åŠ›å¼ºï¼‰
        3. éƒ½å¤±è´¥åˆ™å½’ç±»ä¸º Other

    å±æ€§è¯´æ˜ï¼š
        categories: æ”¯æŒçš„åˆ†ç±»ç±»åˆ«åˆ—è¡¨
        rules: å…³é”®è¯è§„åˆ™å­—å…¸ {ç±»åˆ«: [å…³é”®è¯åˆ—è¡¨]}
        model: æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆæœ´ç´ è´å¶æ–¯ï¼‰
        vectorizer: TF-IDF å‘é‡åŒ–å™¨
    """

    # ==================== ç±»å¸¸é‡ ====================
    # ä½¿ç”¨ç±»å¸¸é‡å®šä¹‰é»˜è®¤ç±»åˆ«ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†
    CATEGORY_NEWS = "News"           # æ–°é—»
    CATEGORY_ENTERTAINMENT = "Entertainment"  # å¨±ä¹
    CATEGORY_LEARNING = "Learning"   # å­¦ä¹ 
    CATEGORY_SOCIAL = "Social"        # ç¤¾äº¤
    CATEGORY_SHOPPING = "Shopping"    # è´­ç‰©
    CATEGORY_TOOLS = "Tools"          # å·¥å…·
    CATEGORY_OTHER = "Other"          # å…¶ä»–

    # ==================== åˆå§‹åŒ–æ–¹æ³• ====================

    def __init__(self,
                 keyword_dict: Optional[Dict[str, List[str]]] = None,
                 model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨

        å‚æ•°:
            keyword_dict: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼Œæ ¼å¼ä¸º {'ç±»åˆ«': ['è¯1', 'è¯2']}
                          å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤è§„åˆ™åº“
            model_path: å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨åŠ è½½

        å­¦ä¹ è¦ç‚¹:
            - Optional ç±»å‹æç¤ºè¡¨ç¤ºå‚æ•°å¯ä»¥ä¸º None
            - __init__ æ–¹æ³•ä¸åº”åŒ…å«è€—æ—¶æ“ä½œ
        """
        # TODO: åˆå§‹åŒ–åˆ†ç±»ç±»åˆ«åˆ—è¡¨
        self.categories = []  # è¯·å¡«å……æ‰€æœ‰ç±»åˆ«

        # TODO: åŠ è½½å…³é”®è¯è§„åˆ™åº“
        self.rules = {}  # è°ƒç”¨ _load_default_rules() æˆ–ä½¿ç”¨ä¼ å…¥çš„ keyword_dict

        # TODO: åˆå§‹åŒ–æœºå™¨å­¦ä¹ ç›¸å…³å±æ€§ï¼ˆåˆå§‹ä¸º Noneï¼‰
        self.model = None       # æœ´ç´ è´å¶æ–¯æ¨¡å‹
        self.vectorizer = None  # TF-IDF å‘é‡åŒ–å™¨

        # TODO: å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œå°è¯•åŠ è½½æ¨¡å‹
        # æç¤ºï¼šè°ƒç”¨ self.load_model(model_path)

        print("âœ… ContentClassifier åˆå§‹åŒ–å®Œæˆ")

    # ==================== ç§æœ‰æ–¹æ³•ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰====================

    def _load_default_rules(self) -> Dict[str, List[str]]:
        """
        åŠ è½½é»˜è®¤çš„å…³é”®è¯è§„åˆ™åº“

        è¿”å›:
            Dict[str, List[str]]: å…³é”®è¯è§„åˆ™å­—å…¸

        è®¾è®¡å»ºè®®:
            1. å…³é”®è¯è¦å…·æœ‰ä»£è¡¨æ€§ï¼Œé¿å…æ­§ä¹‰
            2. å¯ä»¥åŒæ—¶åŒ…å«ä¸­æ–‡å’Œè‹±æ–‡å…³é”®è¯
            3. URL åŸŸåå…³é”®è¯æ¯”æ ‡é¢˜å…³é”®è¯æ›´å‡†ç¡®

        TODO: è¯·æ ¹æ®æ‚¨çš„æµè§ˆä¹ æƒ¯å¡«å……å…³é”®è¯
        """
        return {
            self.CATEGORY_SOCIAL: ["QQ", "å¾®ä¿¡", "çŸ¥ä¹", "ç™¾åº¦è´´å§", "Weibo", "Bilibili", "æŠ–éŸ³", "å¾®åš", "å°çº¢ä¹¦",
                                   "å¿«æ‰‹", "é™Œé™Œ", "æ¢æ¢", "è±†ç“£", "é¢†è‹±", "è„‰è„‰", "æœ‹å‹åœˆ", "ç¾¤èŠ", "è®ºå›", "ç¤¾åŒº",
                                   "Instagram", "Facebook", "Twitter", "Telegram", "Discord", "Reddit"],
            self.CATEGORY_LEARNING: ["æ•™ç¨‹", "ä»é›¶å¼€å§‹", "æ–‡æ¡£", "æ–°æ‰‹", "æ•°å­¦", "ç‰©ç†", "ä»£æ•°", "ç»ƒä¹ ", "python",
                                     "stackoverflow", "ç¼–ç¨‹", "ç®—æ³•", "è®¡ç®—æœº", "è‹±è¯­", "å››å…­çº§", "é›…æ€", "æ‰˜ç¦",
                                     "è€ƒç ”", "å…¬åŠ¡å‘˜", "è¯¾ç¨‹", "Java", "GitHub", "CSDN", "LeetCode", "ç»´åŸºç™¾ç§‘",
                                     "Coursera"],
            self.CATEGORY_SHOPPING: ["æ·˜å®", "äº¬ä¸œ", "PDD", "æ‹¼å¤šå¤š", "è´­ç‰©è½¦", "ä»·æ ¼", "ç§’æ€", "å¤©çŒ«", "è‹å®",
                                     "ä¼˜æƒ åˆ¸", "æŠ˜æ‰£", "åŒ…é‚®", "ä¸‹å•", "ç‰©æµ", "å¿«é€’", "å•†åŸ", "ä¿ƒé”€", "å›¢è´­", "æ»¡å‡",
                                     "äºšé©¬é€Š", "å”¯å“ä¼š", "å¾—ç‰©", "é—²é±¼", "æ”¯ä»˜å®"],
            self.CATEGORY_ENTERTAINMENT: ["ç”µå½±", "å°è¯´", "è¿½ç•ª", "æ¼«ç”»", "æ¸¸æˆ", "éŸ³ä¹", "music", "ç”µè§†å‰§", "ç»¼è‰º",
                                          "åŠ¨æ¼«", "ä½“è‚²", "ç›´æ’­", "æç¬‘", "å¨±ä¹", "æ˜æ˜Ÿ", "å…«å¦", "å½±è§†", "Netflix",
                                          "YouTube", "ç½‘æ˜“äº‘", "QQéŸ³ä¹", "çˆ±å¥‡è‰º", "è…¾è®¯è§†é¢‘", "ä¼˜é…·", "Steam",
                                          "Twitch"],
            self.CATEGORY_NEWS: ["æ–°é—»", "æ—¥æŠ¥", "å¤´æ¡", "å¤®è§†", "èµ„è®¯", "æ—¶æ”¿", "å›½é™…", "è´¢ç»", "ç§‘æŠ€", "ç¤¾ä¼š", "å†›äº‹",
                                 "çƒ­ç‚¹", "çªå‘", "æ—©æŠ¥", "æ™šæŠ¥", "æ–°æµª", "æ¾æ¹ƒ", "ä»Šæ—¥å¤´æ¡", "BBC", "CNN", "ç¯çƒæ—¶æŠ¥",
                                 "å‚è€ƒæ¶ˆæ¯"],
            self.CATEGORY_TOOLS: ["ç¿»è¯‘", "ç½‘ç›˜", "é‚®ç®±", "åœ°å›¾", "è®¡ç®—å™¨", "å¤©æ°”", "æ—¥å†", "æ—¶é’Ÿ", "é—¹é’Ÿ", "è®°äº‹æœ¬",
                                  "ä¾¿ç­¾", "å½•éŸ³", "æˆªå›¾", "å‹ç¼©", "è§£å‹", "PDF", "æ ¼å¼è½¬æ¢", "
        }

    def _segment_text(self, text: str) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯

        å‚æ•°:
            text: å¾…åˆ†è¯çš„æ–‡æœ¬å­—ç¬¦ä¸²

        è¿”å›:
            List[str]: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨

        å­¦ä¹ è¦ç‚¹:
            - jieba.cut() è¿”å›ç”Ÿæˆå™¨ï¼Œéœ€è¦ç”¨ list() è½¬æ¢
            - jieba.lcut() ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œæ›´æ–¹ä¾¿
            - å¯ä»¥åŠ è½½è‡ªå®šä¹‰è¯å…¸æé«˜åˆ†è¯å‡†ç¡®ç‡

        jieba å¸¸ç”¨æ–¹æ³•:
            - jieba.cut(text): ç²¾ç¡®æ¨¡å¼åˆ†è¯
            - jieba.lcut(text): è¿”å›åˆ—è¡¨
            - jieba.add_word(word): æ·»åŠ è‡ªå®šä¹‰è¯
            - jieba.load_userdict(path): åŠ è½½è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶
        """
        # TODO: å®ç°åˆ†è¯é€»è¾‘
        # æç¤ºï¼šå¤„ç† text ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²çš„æƒ…å†µ
        pass

    def _remove_stopwords(self, words: List[str]) -> List[str]:
        """
        ç§»é™¤åœç”¨è¯

        å‚æ•°:
            words: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨

        è¿”å›:
            List[str]: ç§»é™¤åœç”¨è¯åçš„è¯è¯­åˆ—è¡¨

        è¯´æ˜:
            åœç”¨è¯æ˜¯æŒ‡"çš„"ã€"æ˜¯"ã€"åœ¨"ç­‰æ— å®é™…æ„ä¹‰çš„è¯è¯­
            ç§»é™¤åœç”¨è¯å¯ä»¥æé«˜åˆ†ç±»å‡†ç¡®ç‡

        TODO:
            1. å®šä¹‰åœç”¨è¯åˆ—è¡¨æˆ–ä»æ–‡ä»¶åŠ è½½
            2. è¿‡æ»¤æ‰åœç”¨è¯
        """
        pass

    def _extract_domain(self, url: str) -> str:
        """
        ä» URL ä¸­æå–åŸŸå

        å‚æ•°:
            url: å®Œæ•´çš„ URL å­—ç¬¦ä¸²

        è¿”å›:
            str: åŸŸåéƒ¨åˆ†ï¼Œå¦‚ "www.baidu.com"

        æç¤º:
            - å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²çš„ split('/') æ–¹æ³•
            - æˆ–ä½¿ç”¨ urllib.parse.urlparse() è§£æ

        ç¤ºä¾‹:
            è¾“å…¥: "https://www.bilibili.com/video/xxx"
            è¾“å‡º: "www.bilibili.com" æˆ– "bilibili.com"
        """
        # TODO: å®ç°åŸŸåæå–
        pass

    def _predict_by_model(self, text: str) -> str:
        """
        ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹

        å‚æ•°:
            text: å¾…é¢„æµ‹çš„æ–‡æœ¬

        è¿”å›:
            str: é¢„æµ‹çš„ç±»åˆ«

        å‰ç½®æ¡ä»¶:
            self.model å’Œ self.vectorizer å¿…é¡»å·²è®­ç»ƒ

        å®ç°æ­¥éª¤:
            1. å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œé¢„å¤„ç†
            2. ä½¿ç”¨ vectorizer è½¬æ¢ä¸º TF-IDF å‘é‡
            3. ä½¿ç”¨ model.predict() é¢„æµ‹ç±»åˆ«
        """
        # TODO: Day 4 å®ç°æœºå™¨å­¦ä¹ é¢„æµ‹
        pass

    # ==================== æ ¸å¿ƒå…¬å…±æ–¹æ³• ====================

    def predict_by_rules(self, text: str, url: Optional[str] = None) -> Optional[str]:
        """
        åŸºäºå…³é”®è¯è§„åˆ™è¿›è¡Œåˆ†ç±»

        å‚æ•°:
            text: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µ URLï¼ˆå¯é€‰ï¼Œè¾…åŠ©åˆ¤æ–­ï¼‰

        è¿”å›:
            Optional[str]: åŒ¹é…åˆ°çš„ç±»åˆ«ï¼ŒæœªåŒ¹é…è¿”å› None

        è®¾è®¡æ€è·¯:
            1. URL åŒ¹é…ä¼˜å…ˆï¼ˆåŸŸåæ›´å‡†ç¡®ï¼‰
            2. æ ‡é¢˜å…³é”®è¯åŒ¹é…æ¬¡ä¹‹
            3. åªè¦åŒ¹é…åˆ°å°±è¿”å›ï¼Œä¸è¿›è¡Œå¤šç±»åˆ«åˆ¤æ–­

        æç¤º:
            - å­—ç¬¦ä¸²çš„ in æ“ä½œç¬¦å¯ä»¥åˆ¤æ–­å­ä¸²
            - ä½¿ç”¨ .lower() ç»Ÿä¸€è½¬ä¸ºå°å†™ï¼Œæé«˜åŒ¹é…ç‡
        """
        # TODO: å®ç°è§„åˆ™åŒ¹é…é€»è¾‘
        # ä¼ªä»£ç æ¡†æ¶ï¼š
        # if url å­˜åœ¨:
        #     for category, keywords in self.rules.items():
        #         for keyword in keywords:
        #             if keyword åœ¨ url ä¸­:
        #                 return category
        #
        # for category, keywords in self.rules.items():
        #     for keyword in keywords:
        #         if keyword åœ¨ text ä¸­:
        #             return category
        #
        # return None
        pass

    def train_model(self,
                    texts: List[str],
                    labels: List[str],
                    test_size: float = 0.2) -> Dict[str, float]:
        """
        è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

        å‚æ•°:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            labels: å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨
            test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼Œé»˜è®¤ 0.2

        è¿”å›:
            Dict[str, float]: åŒ…å«å‡†ç¡®ç‡ç­‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸

        å®ç°æ­¥éª¤:
            1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆåˆ†è¯ã€å»åœç”¨è¯ï¼‰
            2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            3. åˆ›å»º TfidfVectorizer å¹¶è½¬æ¢æ–‡æœ¬
            4. è®­ç»ƒ MultinomialNB æ¨¡å‹
            5. è¯„ä¼°æ¨¡å‹æ€§èƒ½

        sklearn å…³é”®æ–¹æ³•:
            - TfidfVectorizer(): åˆ›å»º TF-IDF å‘é‡åŒ–å™¨
              - fit_transform(texts): æ‹Ÿåˆå¹¶è½¬æ¢
              - transform(texts): ä»…è½¬æ¢ï¼ˆç”¨äºæ–°æ•°æ®ï¼‰
            - MultinomialNB(): æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
              - fit(X, y): è®­ç»ƒæ¨¡å‹
              - predict(X): é¢„æµ‹
              - score(X, y): è®¡ç®—å‡†ç¡®ç‡
            - train_test_split(): åˆ’åˆ†æ•°æ®é›†

        TODO: Day 4 å®ç°æ¨¡å‹è®­ç»ƒ
        """
        print("ğŸ”„ æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
        # TODO: å®ç°è®­ç»ƒé€»è¾‘
        pass

    def predict(self, text: str, url: Optional[str] = None) -> str:
        """
        é¢„æµ‹å•æ¡æ–‡æœ¬çš„ç±»åˆ«ï¼ˆä¸»å…¥å£æ–¹æ³•ï¼‰

        å‚æ•°:
            text: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µ URLï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            str: é¢„æµ‹çš„ç±»åˆ«

        åˆ†ç±»ç­–ç•¥:
            1. ä¼˜å…ˆä½¿ç”¨è§„åˆ™åŒ¹é…ï¼ˆå¿«é€Ÿã€å‡†ç¡®ï¼‰
            2. è§„åˆ™æœªå‘½ä¸­ä¸”æœ‰æ¨¡å‹æ—¶ï¼Œä½¿ç”¨æ¨¡å‹é¢„æµ‹
            3. éƒ½å¤±è´¥åˆ™è¿”å› Other

        è¿™æ˜¯ç±»æœ€é‡è¦çš„å¯¹å¤–æ¥å£ï¼
        """
        # TODO: å®ç°é¢„æµ‹é€»è¾‘
        # æç¤ºï¼šè°ƒç”¨ self.predict_by_rules() å’Œ self._predict_by_model()
        pass

    def batch_predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ‰¹é‡é¢„æµ‹ DataFrame ä¸­çš„æ•°æ®

        å‚æ•°:
            df: åŒ…å« 'title' å’Œ 'url' åˆ—çš„ DataFrame

        è¿”å›:
            pd.DataFrame: æ·»åŠ äº† 'category' åˆ—çš„ DataFrame

        Pandas æŠ€å·§:
            - df.apply(func, axis=1): å¯¹æ¯è¡Œåº”ç”¨å‡½æ•°
            - df['col'].apply(func): å¯¹å•åˆ—åº”ç”¨å‡½æ•°
            - ä½¿ç”¨è¿›åº¦æ¡åº“ tqdm å¯ä»¥æ˜¾ç¤ºå¤„ç†è¿›åº¦

        æ€§èƒ½ä¼˜åŒ–å»ºè®®:
            - å¯¹äºå¤§é‡æ•°æ®ï¼Œå¯ä»¥è€ƒè™‘å‘é‡åŒ–æ“ä½œ
            - æˆ–è€…ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        """
        if df.empty:
            print("âš ï¸ è¾“å…¥æ•°æ®ä¸ºç©º")
            return df

        print(f"ğŸ“Š æ­£åœ¨å¤„ç† {len(df)} æ¡æ•°æ®...")

        # TODO: å®ç°æ‰¹é‡é¢„æµ‹
        # æç¤ºï¼šä½¿ç”¨ df.apply(lambda row: self.predict(...), axis=1)

        return df

    # ==================== æ¨¡å‹æŒä¹…åŒ–æ–¹æ³• ====================

    def save_model(self, path: str) -> None:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æ–‡ä»¶

        å‚æ•°:
            path: æ¨¡å‹ä¿å­˜è·¯å¾„

        è¯´æ˜:
            æ¨¡å‹ä¿å­˜åï¼Œä¸‹æ¬¡å¯åŠ¨å¯ä»¥ç›´æ¥åŠ è½½ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ

        Python æŒä¹…åŒ–æ–¹æ³•:
            - pickle.dump(obj, file): åºåˆ—åŒ–å¯¹è±¡
            - pickle.load(file): ååºåˆ—åŒ–å¯¹è±¡
            - ä¹Ÿå¯ä»¥ä½¿ç”¨ joblibï¼ˆsklearn æ¨èï¼‰

        éœ€è¦ä¿å­˜çš„å†…å®¹:
            - self.model (åˆ†ç±»å™¨)
            - self.vectorizer (å‘é‡åŒ–å™¨)
            - self.categories (ç±»åˆ«åˆ—è¡¨)
        """
        # TODO: å®ç°æ¨¡å‹ä¿å­˜
        # æç¤ºï¼š
        # with open(path, 'wb') as f:
        #     pickle.dump({...}, f)
        pass

    def load_model(self, path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹

        å‚æ•°:
            path: æ¨¡å‹æ–‡ä»¶è·¯å¾„

        è¿”å›:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ

        æ³¨æ„:
            åŠ è½½å‰æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        # TODO: å®ç°æ¨¡å‹åŠ è½½
        pass

    def get_category_distribution(self, df: pd.DataFrame) -> pd.Series:
        """
        ç»Ÿè®¡åˆ†ç±»ç»“æœçš„åˆ†å¸ƒæƒ…å†µ

        å‚æ•°:
            df: åŒ…å« 'category' åˆ—çš„ DataFrame

        è¿”å›:
            pd.Series: å„ç±»åˆ«çš„æ•°é‡ç»Ÿè®¡

        ç”¨é€”:
            ç”¨äºåˆ†æç”¨æˆ·æµè§ˆä¹ æƒ¯ï¼Œç”ŸæˆæŠ¥å‘Š

        Pandas æ–¹æ³•:
            - df['col'].value_counts(): ç»Ÿè®¡å„å€¼å‡ºç°æ¬¡æ•°
        """
        # TODO: å®ç°ç»Ÿè®¡é€»è¾‘
        pass


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    """
    å•å…ƒæµ‹è¯•ï¼šç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ¥æµ‹è¯•åˆ†ç±»å™¨åŠŸèƒ½
    
    æµ‹è¯•æ­¥éª¤:
        1. å®ä¾‹åŒ–åˆ†ç±»å™¨
        2. æµ‹è¯•å•æ¡æ–‡æœ¬é¢„æµ‹
        3. æµ‹è¯•æ‰¹é‡é¢„æµ‹
        4. (å¯é€‰) æµ‹è¯•æ¨¡å‹è®­ç»ƒå’Œä¿å­˜
    """

    print("=" * 50)
    print("ContentClassifier å•å…ƒæµ‹è¯•")
    print("=" * 50)

    # TODO: ç¼–å†™æµ‹è¯•ä»£ç 

    # 1. å®ä¾‹åŒ–åˆ†ç±»å™¨
    classifier = ContentClassifier()

    # 2. æµ‹è¯•å•æ¡é¢„æµ‹
    # test_cases = [
    #     {"title": "Python åŸºç¡€æ•™ç¨‹", "url": "https://www.runoob.com/python"},
    #     {"title": "äº¬ä¸œ - æ­£å“ä½ä»·", "url": "https://www.jd.com"},
    #     ...
    # ]
    # for case in test_cases:
    #     result = classifier.predict(case['title'], case['url'])
    #     print(f"æ ‡é¢˜: {case['title']} -> {result}")

    # 3. æµ‹è¯•æ‰¹é‡é¢„æµ‹
    # df = pd.DataFrame(test_cases)
    # result_df = classifier.batch_predict(df)
    # print(result_df)

    print("\nâœ… æµ‹è¯•å®Œæˆ")